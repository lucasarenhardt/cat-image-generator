# Cat Faces Image Generation with GANs

This project uses Generative Adversarial Networks (GANs) to generate images of cat faces. Built with TensorFlow and Keras, this model leverages deep convolutional architectures in both the generator and discriminator networks. The project includes evaluation of image quality using the Frechet Inception Distance (FID), a common metric for assessing GAN performance.

## Features

- Utilizes Convolutional Neural Networks (CNNs) for cat image generation.
- Loads and processes a dataset of cat faces for training.
- Employs a GAN architecture, where:
    - The generator learns to create realistic cat face images.
    - The discriminator evaluate the authencity of generated cat images.
- Trains the model using a GPU (if available).

## Dataset
The project uses a collection of cat face images, which are preprocessed and normalized to fit the model's requirements.

- **Source**: [Animal Faces HQ Dataset (AFHQ)](https://github.com/clovaai/stargan-v2/tree/master), using only the cat subset.
- Images are resized to 128x128 pixels and normalized to a range between 0 and 1 for efficient training.

## Model Architecture

The GAN model is built using TensorFlow and Keras libraries, with distinct architectures for the generator and discriminator:

- Generator Architecture: The generator network uses layers such as convolutional transpose layers, batch normalization, and LeakyReLU activations to create synthetic images from random noise vectors.
- Discriminator Architecture: The discriminator network consists of convolutional layers with dropout and LeakyReLU activations, designed to classify images as real or fake.

The adversarial training approach helps the generator and discriminator improve simultaneously, refining the quality of generated images over successive training epochs.

## Frechet Inception Distance (FID)

Frechet Inception Distance (FID) is a widely-used metric for evaluating the quality of images generated by GANs. FID assesses the similarity between real and generated images by comparing their feature distributions in a deep feature space, typically using an Inception-v3 model. Lower FID values indicate higher image quality.
